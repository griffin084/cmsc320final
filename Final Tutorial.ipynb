{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49202953-1725-4958-bc27-3d8bcfdc27a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: htmldate in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /Users/griffinanderson/Library/Python/3.10/lib/python/site-packages (from htmldate) (4.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/griffinanderson/Library/Python/3.10/lib/python/site-packages (from htmldate) (2.8.2)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from htmldate) (1.1.4)\n",
      "Requirement already satisfied: urllib3<2,>=1.26 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from htmldate) (1.26.12)\n",
      "Collecting charset-normalizer>=3.0.1\n",
      "  Using cached charset_normalizer-3.0.1-cp310-cp310-macosx_11_0_arm64.whl (122 kB)\n",
      "Requirement already satisfied: tzlocal in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate) (4.2)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate) (2022.2.1)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from dateparser>=1.1.2->htmldate) (2022.10.31)\n",
      "Requirement already satisfied: six>=1.5 in /Users/griffinanderson/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->htmldate) (1.16.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tzlocal->dateparser>=1.1.2->htmldate) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->dateparser>=1.1.2->htmldate) (2022.7)\n",
      "Installing collected packages: charset-normalizer\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.1.1\n",
      "    Uninstalling charset-normalizer-2.1.1:\n",
      "      Successfully uninstalled charset-normalizer-2.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.28.1 requires charset-normalizer<3,>=2, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-3.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/griffinanderson/Library/Python/3.10/lib/python/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/griffinanderson/Library/Python/3.10/lib/python/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (1.26.12)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->transformers) (2022.6.15)\n",
      "Installing collected packages: charset-normalizer\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.0.1\n",
      "    Uninstalling charset-normalizer-3.0.1:\n",
      "      Successfully uninstalled charset-normalizer-3.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "htmldate 1.4.0 requires charset-normalizer>=3.0.1, but you have charset-normalizer 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed charset-normalizer-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: htmldate[all]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProsusAI/finbert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProsusAI/finbert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1009\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1009\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:997\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    995\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "#importing the necessary libraries \n",
    "import requests \n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "import html \n",
    "from bs4 import BeautifulSoup \n",
    "!pip install htmldate\n",
    "!pip install transformers\n",
    "!pip install -U htmldate[all]\n",
    "from htmldate import find_date\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.dates import DateFormatter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d448246-8e2b-4396-84d1-d2a455481fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_programe(ticker,company):\n",
    "    #Specifying the skeleton of the html link for all stocks on marketwatch.com with just the ticker being different for each stock\n",
    "    news_url = 'https://www.marketwatch.com/investing/stock/' + ticker + '?mod=quote_search'\n",
    "    links, dates = scrape_news_links(news_url)\n",
    "    dates_list, positive_score, neutral_score, negative_score, Positive_sum, Neutral_sum, Negative_sum,df,links_final = scrape_news_text(links,dates)\n",
    "    plot_plots(dates_list, positive_score, neutral_score, negative_score, Positive_sum, Neutral_sum, Negative_sum,df,links_final)\n",
    "\n",
    "\n",
    "\n",
    "def scrape_news_links(news_url):\n",
    "    \n",
    "    #Getting the html code for the webpage pertaining to the html link \n",
    "    html = requests.get(news_url).content\n",
    "    \n",
    "    #Converting the html code to a beautiful soup object \n",
    "    soup = BeautifulSoup(html , 'lxml')\n",
    "\n",
    "    #Getting a list of all links on the webpage \n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    #Searching for the destination of the required hyperlinks on the webpage \n",
    "    urls = []\n",
    "    for link in links:\n",
    "        urls.append(link.get('href'))\n",
    "\n",
    "    #Selecting the links that are not None \n",
    "    urls1 = []\n",
    "    for url in urls:\n",
    "        if url is not None:\n",
    "           urls1.append(url)\n",
    "\n",
    "    #Selectng the urls that begin with \"https\" \n",
    "    news_urls = []\n",
    "    substring = 'https://'\n",
    "    substring1 = 'fool.com'\n",
    "    for url in urls1:\n",
    "        if substring in url and substring1 not in url:\n",
    "           news_urls.append(url)\n",
    "    \n",
    "    #Selecting the urls that pertain to the stock articles of the required company \n",
    "    company_news_urls = []\n",
    "    substring2 = company\n",
    "    for url in news_urls:\n",
    "        if substring2 in url:\n",
    "           company_news_urls.append(url)\n",
    "\n",
    "    #Removing the duplicate urls for any given article link scrapped\n",
    "    links_unique = []\n",
    "    for x in company_news_urls:\n",
    "        if x not in links_unique and x is not None:\n",
    "           links_unique.append(x)\n",
    "\n",
    "    dates = []\n",
    "    \n",
    "    #Getting the publishing dates of the scrapped articles \n",
    "    for link in links_unique:\n",
    "        date = find_date(link)\n",
    "        dates.append(date)\n",
    "\n",
    "    return links_unique,dates\n",
    "\n",
    "def scrape_news_text(links_unique,dates):\n",
    "\n",
    "    #Initializing a list to store the news text present in the scrapped links\n",
    "    news_text_list = []\n",
    "    for url in links_unique:\n",
    "\n",
    "        #Getting the html code for the webpages that have the news text \n",
    "        r = requests.get(url,headers = {'User-Agent': 'Popular browser\\'s user-agent',}).content\n",
    "        \n",
    "        #Handling the html code for these news articles using the lxml interface to parse the content to beautiful soup\n",
    "        news_soup = BeautifulSoup(r , 'lxml')\n",
    "     \n",
    "        #Locating the different paragraphs in the article and concatinating them into a string paragraph\n",
    "        paragraphs = [par.text for par in news_soup.find_all('p')]\n",
    "        news_text = '\\n'.join(paragraphs)\n",
    "    \n",
    "        #Appending the final news text into the news text list \n",
    "        news_text_list.append(news_text)\n",
    "\n",
    "    #Creating a pandas dataframe to store data \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Creating a column to store the links for the various scrapped articles\n",
    "    df['News links'] = links_unique\n",
    "\n",
    "    #Creating a column that stores the news text of the various articles\n",
    "    df['News text'] = news_text_list\n",
    "\n",
    "    #Creating a column to store the publishing fdate of the scrapped articles \n",
    "    df['Date'] = dates\n",
    "\n",
    "    #Basic data cleaning steps \n",
    "    for index, row in df.iterrows():\n",
    "        #Removing newline characters\n",
    "        row['News text'] = row['News text'].replace(\"\\n\", \"\")\n",
    "        #Removing whitespace characters\n",
    "        row['News text'] = \" \".join(row['News text'].split())\n",
    "        #Removing links that are not relavant to the text\n",
    "        row['News text'] = re.sub(r'http\\S+', '', row['News text'])\n",
    "\n",
    "    #Tokenizing the words to help the NLP model in interpreting the meaning of the text by analyzing the sequence of the words.\n",
    "    inputs = tokenizer(news_text_list, padding = True, truncation = True, return_tensors='pt')\n",
    "\n",
    "    #Running the tokenized text through the FinBERT NLP model \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    #Generating the sentiment score predictions for these news articles \n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    #Storing the positive, negatibve and neutral scores for each article into seperate lists\n",
    "    positive = predictions[:, 0].tolist()\n",
    "    negative = predictions[:, 1].tolist()\n",
    "    neutral = predictions[:, 2].tolist()\n",
    "\n",
    "    #Creating a dataframe that has the news text, publishing date and the respective sentiment scores on it \n",
    "    table = {'News Text':news_text_list,\n",
    "         \"Positive\":positive,\n",
    "         \"Negative\":negative, \n",
    "         \"Neutral\":neutral}\n",
    "    df = pd.DataFrame(table, columns = [\"News Text\", \"Date\", \"Positive\", \"Negative\", \"Neutral\"])\n",
    "    df['Date'] = dates\n",
    "    df['Article link'] = links_unique\n",
    "\n",
    "    #Finding the average of the different sentiment scores for all the scrapped articles \n",
    "    Negative_score = df['Negative'].values.tolist()\n",
    "    Negative_sum = sum(Negative_score)\n",
    "    Negative_sum /= df.shape[0]\n",
    "    Positive_score = df['Positive'].values.tolist()\n",
    "    Positive_sum = sum(Positive_score)\n",
    "    Positive_sum /= df.shape[0]\n",
    "    Neutral_score = df['Neutral'].values.tolist()\n",
    "    Neutral_sum = sum(Neutral_score)\n",
    "    Neutral_sum /= df.shape[0]\n",
    "\n",
    "    #Sorting the data in the data frame by date of publication \n",
    "    df = df.sort_values(by ='Date')\n",
    "\n",
    "    #Converting the dates to a date-time object and storing the scores and dates in respective lists \n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "    negative_score = df['Negative']\n",
    "    positive_score = df['Positive']\n",
    "    neutral_score = df['Neutral']\n",
    "    dates_list = df['Date']\n",
    "    links_final = df['Article link']\n",
    "    return dates_list, positive_score, neutral_score, negative_score, Positive_sum, Neutral_sum, Negative_sum,df,links_final\n",
    "\n",
    "def plot_plots(dates_list, positive_score, neutral_score, negative_score, Positive_sum, Neutral_sum, Negative_sum,df,links_final):\n",
    "\n",
    "    #Creating a plot using matplotlib\n",
    "    plt.figure(figsize = (20,6))\n",
    "\n",
    "    #getting the number of x axis values that need to be plotted and specifying the width of the bars in the plot\n",
    "    N = len(dates_list)\n",
    "    ind = np.arange(N) \n",
    "    width = 0.25\n",
    "  \n",
    "    #Representing the positive scores in a green bar \n",
    "    xvals = positive_score\n",
    "    bar1 = plt.bar(ind, xvals, width, color = 'g')\n",
    "\n",
    "    #Representing the neutral scores in a blue bar \n",
    "    zvals = neutral_score\n",
    "    bar3 = plt.bar(ind+width, zvals, width, color = 'b')\n",
    "\n",
    "    #Representing the negative scores in a red bar \n",
    "    yvals = negative_score\n",
    "    bar2 = plt.bar(ind+width*2, yvals, width, color='r')\n",
    "  \n",
    "    #Labelling the axis and the title of the plot\n",
    "    plt.xlabel(\"Dates\")\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(\"Sentiment Scores\")\n",
    "\n",
    "    #Making certain orientation adjustments and creating a legend for the plot\n",
    "    plt.xticks(ind+width,dates_list)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.legend((bar1, bar3, bar2), ('Positive Score', 'Neutral Score', 'Negative Score'))\n",
    "    plt.show()\n",
    "    \n",
    "    #Printing a newline\n",
    "    print()\n",
    "\n",
    "    #Printing the dates and links in the dataframe\n",
    "    for i in range (0,len(dates_list)):\n",
    "         print(dates_list[i].strftime('%Y-%m-%d') + ' , ' + links_final[i] + \"\\n\")\n",
    "\n",
    "    #Printing a newline\n",
    "    print()\n",
    "\n",
    "    plt.figure(2, figsize = (20,6))\n",
    "      #getting the number of x axis values that need to be plotted and specifying the width of the bars in the plot\n",
    "    M = 1\n",
    "    ind = np.arange(M) \n",
    "    width = 0.7\n",
    "  \n",
    "    #Representing the positive scores in a green bar \n",
    "    xvals = Positive_sum\n",
    "    bar1 = plt.bar(ind, xvals, width, color = 'g')\n",
    "\n",
    "    #Representing the neutral scores in a blue bar \n",
    "    zvals = Neutral_sum\n",
    "    bar3 = plt.bar(ind+width, zvals, width, color = 'b')\n",
    "\n",
    "    #Representing the negative scores in a red bar \n",
    "    yvals = Negative_sum\n",
    "    bar2 = plt.bar(ind+width*2, yvals, width, color='r')\n",
    "  \n",
    "    #Labelling the axis and the title of the plot\n",
    "    plt.xlabel(\"Date range from \" + str(dates_list[0]) + \" to \" + str(dates_list[N-1]))\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(\"Average Sentiment Scores for range \" + str(dates_list[0]) + \"to\" + str(dates_list[N-1]))\n",
    "\n",
    "    #Making certain orientation adjustments and creating a legend for the plot\n",
    "    plt.xticks([])\n",
    "    plt.legend((bar1, bar3, bar2), ('Positive Score', 'Neutral Score', 'Negative Score'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc76b4-dbb2-4915-877a-5587061d33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking the user to input the ticker and company name in lowercase for processing \n",
    "ticker = input(\"Enter the stock ticker in lowercase(examples: amzn, tsla, k): \" )\n",
    "company = input(\"Enter the company name in lowercase(examples: amazon, tesla, kellogg): \")\n",
    "\n",
    "#Calling the fuction that generates the final plot \n",
    "run_programe(ticker,company)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
